<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Example of Conversion to jupyter notebook | Charlie&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="
Source: https://github.com/chuckles201/DiT-Implementation/blob/main/DDPM/explained.ipynb


Explanations
First, in our diffusion model, we define a markovian noising process, where we gradually add noise. Then, we try to predict the noise backwards (with a neural net. that understands images).
We only predict the mean, and fix the variance based on the variance we defined in the forward-noising process (not learning a covariance).
DDPM vs. DDIM
DDIM allows us to formulate diffusion in a non-markovian way, so we can deterministacally sample and add noise. It is basically sampling one noise-vector, and using this to &lsquo;jump&rsquo; to an arbitrary sample, instead of taking steps.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/test_ipynb/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6a415fa7fc28a3325d0b09df302c418b25489af6a1e14001cbcfc5c872331561.css" integrity="sha256-akFfp/woozJdCwnfMCxBiyVImvah4UABy8/FyHIzFWE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/test_ipynb/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"
onload="renderMathInElement(document.body);"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false},
            {left: "\\[", right: "\\]", display: true}
        ],
        throwOnError : false
    });
});
</script>





</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Charlie&#39;s Blog (Alt + H)">Charlie&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/search/" title="Search üîç (Alt &#43; /)" accesskey=/>
                    <span>Search üîç</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories üé®">
                    <span>Categories üé®</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags üè∑Ô∏è">
                    <span>Tags üè∑Ô∏è</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/manifesto/" title="Manifesto üìú">
                    <span>Manifesto üìú</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <meta name="google" content="notranslate">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Example of Conversion to jupyter notebook
    </h1>
    <div class="post-meta"><span title='2024-12-18 21:26:37 -0500 EST'>December 18, 2024</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#explanations" aria-label="Explanations">Explanations</a><ul>
                        
                <li>
                    <a href="#learning-objective" aria-label="Learning Objective">Learning Objective</a></li></ul>
                </li>
                <li>
                    <a href="#learning-variance" aria-label="Learning variance">Learning variance</a><ul>
                        <ul>
                        <ul>
                        
                <li>
                    <a href="#loss-function" aria-label="Loss-function">Loss-function</a></li></ul>
                    </ul>
                    </ul>
                </li>
                <li>
                    <a href="#ddim-inference" aria-label="DDIM inference">DDIM inference</a><ul>
                        
                <li>
                    <a href="#paper-notes" aria-label="Paper-notes">Paper-notes</a><ul>
                        
                <li>
                    <a href="#formula" aria-label="Formula">Formula</a></li>
                <li>
                    <a href="#accelerated-generation-process" aria-label="&lsquo;Accelerated&rsquo; Generation Process">&lsquo;Accelerated&rsquo; Generation Process</a></li>
                <li>
                    <a href="#inference" aria-label="Inference">Inference</a></li>
                <li>
                    <a href="#learned-variance" aria-label="Learned-Variance">Learned-Variance</a><ul>
                        
                <li>
                    <a href="#now-let" aria-label="Now, let&rsquo;s test out this new formulation of inference on our trained DiT model!">Now, let&rsquo;s test out this new formulation of inference on our trained DiT model!</a></li></ul>
                </li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#guidance" aria-label="Guidance">Guidance</a><ul>
                        <ul>
                        
                <li>
                    <a href="#cfg" aria-label="CFG">CFG</a>
                </li>
            </ul>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p>Source: <a href="https://github.com/chuckles201/DiT-Implementation/blob/main/DDPM/explained.ipynb">https://github.com/chuckles201/DiT-Implementation/blob/main/DDPM/explained.ipynb</a></p></blockquote>


<h1 id="explanations">Explanations<a hidden class="anchor" aria-hidden="true" href="#explanations">#</a></h1>
<p>First, in our diffusion model, we define a markovian noising process, where we gradually add noise. Then, we try to predict the noise backwards (with a neural net. that understands images).</p>
<p>We only predict the mean, and fix the variance based on the variance we defined in the forward-noising process (not learning a covariance).</p>
<p><em><strong>DDPM vs. DDIM</strong></em></p>
<p>DDIM allows us to formulate diffusion in a non-markovian way, so we can deterministacally sample and add noise. It is basically sampling one noise-vector, and using this to &lsquo;jump&rsquo; to an arbitrary sample, instead of taking steps.</p>
<p><em><strong>? what is an intuitive explanation for how DDIMs vs DDPMS work, how can we formulate them, understand, and sample from them???</strong></em></p>
<p>Here is a visualization:</p>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torchvision
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span></code></pre></div>
</details>
</div>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># We will be training our neural net. to predict the </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># noise that created an image (which is the same) as</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># predicting the lower noising-step</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#39;img.jpg&#39;</span>)
</span></span><span style="display:flex;"><span>to_tensor <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>ToTensor()
</span></span><span style="display:flex;"><span>image <span style="color:#f92672">=</span> to_tensor(image)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># showing all steps, or just current step</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add_noise</span>(image,steps<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>,current_step<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>,scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.5</span>):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># alphas, betas, alpha_cumprod.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># makes the noising exponentially faster</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># but on same-scale</span>
</span></span><span style="display:flex;"><span>    betas <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0.0001</span><span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>scale),<span style="color:#ae81ff">0.02</span><span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>scale),steps)<span style="color:#f92672">**</span>scale
</span></span><span style="display:flex;"><span>    alphas <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>betas
</span></span><span style="display:flex;"><span>    alphas_cumprod <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumprod(alphas,dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># normal distr.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># DDIM sampling</span>
</span></span><span style="display:flex;"><span>    noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(image)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> current_step <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># having 10x less than steps!</span>
</span></span><span style="display:flex;"><span>        num_rows <span style="color:#f92672">=</span> steps <span style="color:#f92672">//</span> <span style="color:#ae81ff">50</span> <span style="color:#f92672">+</span> steps <span style="color:#f92672">%</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>        fig,subplots <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(num_rows,<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>        fig<span style="color:#f92672">.</span>set_size_inches(<span style="color:#ae81ff">30</span>,<span style="color:#ae81ff">30</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(steps <span style="color:#f92672">//</span> <span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            alpha <span style="color:#f92672">=</span> alphas_cumprod[i<span style="color:#f92672">*</span><span style="color:#ae81ff">10</span>]
</span></span><span style="display:flex;"><span>            beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha
</span></span><span style="display:flex;"><span>            noised_image <span style="color:#f92672">=</span> noise<span style="color:#f92672">*</span>beta<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">+</span> image<span style="color:#f92672">*</span>alpha<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> 
</span></span><span style="display:flex;"><span>            subplots[i <span style="color:#f92672">//</span> <span style="color:#ae81ff">5</span>][i <span style="color:#f92672">%</span> <span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>imshow(noised_image<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>            subplots[i <span style="color:#f92672">//</span> <span style="color:#ae81ff">5</span>][i <span style="color:#f92672">%</span> <span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>i<span style="color:#f92672">*</span><span style="color:#ae81ff">10</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>            subplots[i <span style="color:#f92672">//</span> <span style="color:#ae81ff">5</span>][i <span style="color:#f92672">%</span> <span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        alpha <span style="color:#f92672">=</span> alphas_cumprod[current_step]
</span></span><span style="display:flex;"><span>        beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        noised_image <span style="color:#f92672">=</span> noise<span style="color:#f92672">*</span>beta<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">+</span> image<span style="color:#f92672">*</span>alpha<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> 
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>imshow(noised_image<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>add_noise(image,<span style="color:#ae81ff">1000</span>,<span style="color:#66d9ef">None</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    
</span></span></code></pre></div>
</details>
</div>
<p>
 

 <style>
    .code_output img{
        text-align: center;
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
    }
    .code_output details{
        white-space: pre-wrap !important;
        word-wrap: break-word !important;
        flex-grow: 1;
        padding: 0px;
      }

      

 </style>
 <div class="code_output">
 <pre><code><details class="code_output" open><summary>Output:</summary>
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.034008387..1.0167794].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.13464947..1.070017].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.220642..1.1232466].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.30258492..1.1736095].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.38420695..1.2247505].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.46660542..1.2765263].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.55015445..1.3286602].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.63494563..1.381191].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.720954..1.4340833].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.80809647..1.4872661].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.896262..1.5406518].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.98532385..1.5941445].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.0751463..1.6476442].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.1655916..1.7010512].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.2565187..1.7542658].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.347786..1.807189].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.4392526..1.8597249].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.5307786..1.9117792].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.6222248..1.9658263].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.7134539..2.0295289].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8043313..2.1176076].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.8947227..2.2050211].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9844983..2.2916417].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.0735304..2.3773432].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1616945..2.4620044].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2488706..2.5455081].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3349411..2.6277409].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.4197936..2.708594].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.5033195..2.7879653].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.5854158..2.865756].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.6659844..2.9418743].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.7503884..3.0162334].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.833336..3.0887527].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.9144676..3.1593595].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.993701..3.2279859].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.07096..3.2945719].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.146177..3.3590639].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.2192903..3.421415].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.2902453..3.4815876].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.3589952..3.539549].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.4254994..3.5952742].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.4897263..3.6487465].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.5516505..3.6999557].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.6112545..3.748899].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.6685276..3.7955804].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.7234666..3.8400102].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.7760756..3.8822064].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.826365..3.922193].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.8743522..3.96].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.9200606..3.9956636].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-3.96352..4.0292244].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.0047665..4.0607305].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.04384..4.090232].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.080787..4.1177855].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.1156583..4.1434507].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.148509..4.167291].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.179397..4.1894717].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.208385..4.21492].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.235537..4.238621].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.2609224..4.260645].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.2846084..4.2810616].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.306667..4.299946].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.3271713..4.3173704].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.3461924..4.3334093].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.363805..4.348137].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.380081..4.361627].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.395094..4.3739524].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.408915..4.3851843].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.421616..4.3953943].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.4332643..4.40465].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.4439282..4.413019].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.4536734..4.4205656].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.462563..4.427352].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.470658..4.433437].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.478017..4.4388785].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.484695..4.4437304].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.490746..4.448044].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.49622..4.451867].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.501165..4.455246].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5056252..4.458223].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5096426..4.460837].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.513257..4.463125].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5165043..4.465122].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.519419..4.466859].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5220323..4.4683642].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5243726..4.4696636].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.526467..4.4707823].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.52834..4.471741].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5300126..4.4725604].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5315075..4.4732566].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.532841..4.4738464].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.534031..4.4743443].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5350924..4.474762].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5360384..4.4751105].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5368814..4.4754004].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5376334..4.47564].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5383034..4.4758368].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.538901..4.4759974].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.539433..4.4761276].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.5399065..4.4762316].

&lt;Figure size 3000x3000 with 100 Axes&gt;


<img src="/images/posts/test_ipynb/cell_2_img.png"></details></code></pre>
</div>
<br></p>
<h2 id="learning-objective">Learning Objective<a hidden class="anchor" aria-hidden="true" href="#learning-objective">#</a></h2>
<p>In DDPM&rsquo;s, we want to predict the noise that makes up an image, at any arbitrary step.</p>
<p>We acheive this loss objective, because we are actually trying to predict the mean of the previous noise-step (x_t-1), and this is made up of the ground-truth noise.</p>
<p>So when we plug this into a formula, we can get the mean, and then add back in the variance. And we could then continously sample; but all we care about for now is the training objective:</p>
<ul>
<li>Define a random timestep</li>
<li>Define a noise-vector, add this to the image</li>
<li>Do MSE loss, noise-vector is the target, and the models output given the noisy image, and the timestep is the prediction</li>
</ul>
<p>&gt; Note that the actual loss-formula tells us to weight more important/large noising steps more powerfully, however we ignore this, as at each step we want the model to be robust at predicting the noise, and the step itself shouldn&rsquo;t matter.</p>
<p>The reason we may want to do this, is because at earlier/more-noised steps, we want the model to be able to generate a variety, a of images, and then later on it will hone in on the image that was chosen in an earlier-step.</p>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># very stupid model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;Adding random noise
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">to a batch corresponding to 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">random-timesteps.&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DDPM</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,steps,betas,scale):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>betas <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(betas[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>scale),betas[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>scale),steps) <span style="color:#f92672">**</span> (scale)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alphas <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>betas
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alphas_cumprod <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumprod(self<span style="color:#f92672">.</span>alphas,dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>steps<span style="color:#f92672">=</span> steps
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add_random_noise</span>(self,batch):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># for each batch, add-noise,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># and return noisy image + noise</span>
</span></span><span style="display:flex;"><span>        input_shape <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># easy to deal with</span>
</span></span><span style="display:flex;"><span>        batch <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>view(input_shape[<span style="color:#ae81ff">0</span>],<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(batch)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># broadcast across dim=1</span>
</span></span><span style="display:flex;"><span>        steps <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>,self<span style="color:#f92672">.</span>steps,size<span style="color:#f92672">=</span>(batch<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>],))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># selecting from alpha bars</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B, 1</span>
</span></span><span style="display:flex;"><span>        alpha_bars <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>alphas_cumprod[steps]<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># adding according to noising process</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># std dev and mean</span>
</span></span><span style="display:flex;"><span>        noised_images <span style="color:#f92672">=</span> alpha_bars<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> batch <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha_bars)<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> noise
</span></span><span style="display:flex;"><span>        noised_images <span style="color:#f92672">=</span> noised_images<span style="color:#f92672">.</span>view(input_shape)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># step doesn&#39;t matter; predicting</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># same ground-truth noise!</span>
</span></span><span style="display:flex;"><span>        noise <span style="color:#f92672">=</span> noise<span style="color:#f92672">.</span>view(input_shape)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> noised_images, noise
</span></span><span style="display:flex;"><span>    
</span></span></code></pre></div>
</details>
</div>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># testing out noiser</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># with two noisings</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>images <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack([image,image,image,image],dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>ddpm <span style="color:#f92672">=</span> DDPM(<span style="color:#ae81ff">1000</span>,[<span style="color:#ae81ff">0.0001</span>,<span style="color:#ae81ff">0.02</span>],scale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> ddpm<span style="color:#f92672">.</span>add_random_noise(images)
</span></span><span style="display:flex;"><span>print(out[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig,axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>axes[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>imshow(out[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>axes[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>imshow(out[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>axes[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>imshow(out[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>axes[<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>imshow(out[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">3</span>]<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> ax <span style="color:#f92672">in</span> axes:
</span></span><span style="display:flex;"><span>    ax<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>print(out[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>shape)
</span></span></code></pre></div>
</details>
</div>
<p>
 

 <style>
    .code_output img{
        text-align: center;
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
    }
    .code_output details{
        white-space: pre-wrap !important;
        word-wrap: break-word !important;
        flex-grow: 1;
        padding: 0px;
      }

      

 </style>
 <div class="code_output">
 <pre><code><details class="code_output" open><summary>Output:</summary>
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.6374433..2.9468596].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.089529..2.3521986].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.4482476..2.0029497].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.5749754..2.204132].

torch.Size([4, 3, 280, 280])
torch.Size([4, 3, 280, 280])

&lt;Figure size 640x480 with 4 Axes&gt;


<img src="/images/posts/test_ipynb/cell_5_img.png"></details></code></pre>
</div>
<br></p>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># now, we would just calculate MSE loss </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># between the model and the actual noise!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;As long as we have a good representation of what we want the model to denoise,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">we should be able to test this intuition vs. practice, and set our scale parameter, or
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">use a different parameterization for our model.&#39;&#39;&#39;</span>
</span></span></code></pre></div>
</details>
</div>
<p>
 

 <style>
    .code_output img{
        text-align: center;
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
    }
    .code_output details{
        white-space: pre-wrap !important;
        word-wrap: break-word !important;
        flex-grow: 1;
        padding: 0px;
      }

      

 </style>
 <div class="code_output">
 <pre><code><details class="code_output" open><summary>Output:</summary>
'As long as we have a good representation of what we want the model to denoise,\nwe should be able to test this intuition vs. practice, and set our scale parameter, or\nuse a different parameterization for our model.'
</details></code></pre>
</div>
<br></p>
<p>&gt; So, why DDPM in the first place? Well, since we need to sample from nothing (noise), but we also want to train the model to build-up our image, diffusion is great, because it allows us to understand our image at each step, and allows the model during inference to start by kind of predicting randomly, but gain a &lsquo;feel&rsquo; for what the image is; alowing us to effectively &lsquo;sample&rsquo; from the true distribution of images effectively.</p>
<p>The key is the models training to predict noise at <em>each</em> step</p>
<h1 id="learning-variance"><em><strong>Learning variance</strong></em><a hidden class="anchor" aria-hidden="true" href="#learning-variance">#</a></h1>
<p>In the original DDPM paper, variance was set to either Œ≤ or Œ≤~, which correspond to the upper, and lower-bounds of the variance. The upper-bound works best when q(x_0) is normally distributed, while the lower-bound works when q(x_0) is deterministic at one-point. However, one may ask: why are there even upper or lower bounds on variance, if we know how much noise we are adding (and therefore removing) at each-step?</p>
<p>Here is the original derivation for the upper/lower-bounds:

<img src="/images/posts/test_ipynb/images/form5.jpg"></p>
<p>&gt; How I understand this, is that when we start with a normal distribution, in each step we are removing some of the original noise, and therefore we should add this noise back-in. While for the deterministic case, we remove some of the information (and no-noise), so less-noise should be added.</p>
<p>But in the backward pass (de-noising), shouldn&rsquo;t we just add back the noise we already specified at a given-step (Œ≤)? Well, this does not account for the fact that there may be some noise in the current mean-prediction, or it may be deterministic. If it was deterministic, we wouldn&rsquo;t want to add too much noise, because the mean is more certain; we wouldn&rsquo;t need to capture more distribution due to our uncertainty. This allows our model to be flexible, and &lsquo;over-predict&rsquo; if it deems it necessary, and is sure.</p>
<h4 id="loss-function"><em><strong>Loss-function</strong></em><a hidden class="anchor" aria-hidden="true" href="#loss-function">#</a></h4>
<p>The way that we can predict the variance, is by putting a stop-gradient on the original loss-objective mean, and adding this new term (weighted) to try to learn an optimal variance given a timestep and noisy-image. Here is the formula for the loss:</p>

<img src="/images/posts/test_ipynb/images/form6.png">
<p>where L_VLB is the original loss-function, and we must use this, because it actually involves optimizing our variance-term. The actual loss will be the KL divergence between the two distrbituions q and p, and this should allow the model to learn the variance in the de-noising process to be able to better model the distribution of data.</p>
<p>Remember, when optimizing this, we can simply approximate it with the MSE loss (because gaussian) between the predicted image (with variance), and the actual-image, weighted by some-amount.</p>
<p>our output variance can be learned as:</p>

<img src="/images/posts/test_ipynb/images/form4.png">
<p>where V is the output of our neural network for each latent random-variable. We parameterize it this way so it doesn&rsquo;t explode.</p>
<p>Here are the formulas for our full VLB (remember, we want the KL divergence between the q and p distributions):</p>

<img src="/images/posts/test_ipynb/images/form7.png">

<img src="/images/posts/test_ipynb/images/form8.png">
<p>The intuition for the above formula, is that our ground-truth variance is the &lsquo;actual&rsquo; variance, and any extra variance (bounded by forward-process Œ≤) could help us account for the innacuracy of our mean.</p>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># DDPM w/ hybrid-loss </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (shapes not fully broadcasted,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># see sampler.py for this.)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DDPM</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,steps,betas,scale,loss2_weight<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>loss2_weight <span style="color:#f92672">=</span> loss2_weight
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>betas <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(betas[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>scale),betas[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>scale),steps) <span style="color:#f92672">**</span> scale
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alphas <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>self<span style="color:#f92672">.</span>betas
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alphas_sqrt <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>alphas)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alphas_cumprod <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumprod(self<span style="color:#f92672">.</span>alphas,dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>betas_cumprod <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>alphas_cumprod
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># for quick-compute</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sqrt_alphas_cumprod <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>alphas_cumprod)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>sqrt_betas_cumprod <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>betas_cumprod)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>steps<span style="color:#f92672">=</span> steps
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add_random_noise</span>(self,batch):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># for each batch, add-noise,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># and return noisy image + noise</span>
</span></span><span style="display:flex;"><span>        input_shape <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># easy to deal with</span>
</span></span><span style="display:flex;"><span>        batch <span style="color:#f92672">=</span> batch<span style="color:#f92672">.</span>view(input_shape[<span style="color:#ae81ff">0</span>],<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(batch)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># broadcast across dim=1</span>
</span></span><span style="display:flex;"><span>        steps <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>,self<span style="color:#f92672">.</span>steps,size<span style="color:#f92672">=</span>(batch<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>],))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># selecting from alpha bars</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># B, 1</span>
</span></span><span style="display:flex;"><span>        alpha_bars <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>alphas_cumprod[steps]<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># adding according to noising process</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># std dev and mean</span>
</span></span><span style="display:flex;"><span>        noised_images <span style="color:#f92672">=</span> alpha_bars<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> batch <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha_bars)<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> noise
</span></span><span style="display:flex;"><span>        noised_images <span style="color:#f92672">=</span> noised_images<span style="color:#f92672">.</span>view(input_shape)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># step doesn&#39;t matter; predicting</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># same ground-truth noise!</span>
</span></span><span style="display:flex;"><span>        noise <span style="color:#f92672">=</span> noise<span style="color:#f92672">.</span>view(input_shape)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> noised_images, noise
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hybrid_loss</span>(self,model_out,noise,real_image,t,noised_image):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># getting both of our outputs</span>
</span></span><span style="display:flex;"><span>        pred_noise,pred_var <span style="color:#f92672">=</span> model_out<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">2</span>,dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># simple-loss</span>
</span></span><span style="display:flex;"><span>        l_simple <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mean((pred_noise<span style="color:#f92672">-</span>noise)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># calculating ground-truth mean and</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># predicted-mean (same but diff.-noise)</span>
</span></span><span style="display:flex;"><span>        pred_mean <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>alphas_sqrt[t]) <span style="color:#f92672">*</span> \
</span></span><span style="display:flex;"><span>            (noised_image<span style="color:#f92672">-</span>(self<span style="color:#f92672">.</span>betas[t]<span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>sqrt_betas_cumprod[t])<span style="color:#f92672">*</span>pred_noise)
</span></span><span style="display:flex;"><span>        real_mean <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>alphas_sqrt[t]) <span style="color:#f92672">*</span> \
</span></span><span style="display:flex;"><span>            (noised_image<span style="color:#f92672">-</span>(self<span style="color:#f92672">.</span>betas[t]<span style="color:#f92672">/</span>self<span style="color:#f92672">.</span>sqrt_betas_cumprod[t])<span style="color:#f92672">*</span>noise)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># lower/upper bds of beta</span>
</span></span><span style="display:flex;"><span>        upper_beta <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>betas[t] <span style="color:#75715e"># also &#39;ground-truth&#39;</span>
</span></span><span style="display:flex;"><span>        lower_beta <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>betas[t] <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>betas_cumprod[t<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>betas_cumprod[t]
</span></span><span style="display:flex;"><span>        pred_var <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(torch<span style="color:#f92672">.</span>log(upper_beta)<span style="color:#f92672">*</span>pred_var <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>log(lower_beta)<span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>pred_var))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># full-loss,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># should broadcast to B,shape_img</span>
</span></span><span style="display:flex;"><span>        dkl_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> (lower_beta<span style="color:#f92672">/</span>pred_var <span style="color:#f92672">+</span> (pred_mean<span style="color:#f92672">-</span>real_mean)<span style="color:#f92672">/</span>pred_var <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>log(pred_var<span style="color:#f92672">/</span>lower_beta))
</span></span><span style="display:flex;"><span>        hybrid_loss <span style="color:#f92672">=</span> dkl_loss<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>loss2_weight <span style="color:#f92672">+</span> l_simple
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> hybrid_loss
</span></span><span style="display:flex;"><span>    
</span></span></code></pre></div>
</details>
</div>
<h1 id="ddim-inference"><em><strong>DDIM inference</strong></em><a hidden class="anchor" aria-hidden="true" href="#ddim-inference">#</a></h1>
<p>DDIM allows us to deterministically sample an arbitrary step in the model when we predict the noise at a step. Basically, we define a non-markovian noising process that is exactly equal to our objective in the markovian one (DPPM).</p>
<h2 id="paper-notes"><em><strong>Paper-notes</strong></em><a hidden class="anchor" aria-hidden="true" href="#paper-notes">#</a></h2>
<ul>
<li>
<p>The authors promise a 10x-50x speedup, along with more accurate images, and a way to get deterministic sampling with DDIM.</p>
</li>
<li>
<p>Furthermore, they say that the DDIM inference-formulation allows for similar initial latent-variables (noise) to actually have similar high-level features! This means we can interpolate in latent-space.</p>
</li>
</ul>
<p>The key idea behind DDIM&rsquo;s, stems from the fact that the training objective only relies on the noised image with respect to the original image (q(x_t|x_0)), and not the entire noising process. Therefore, we can actually pick different noising processes for our inference, that still allign with the training objective!</p>
<h3 id="formula"><em><strong>Formula</strong></em><a hidden class="anchor" aria-hidden="true" href="#formula">#</a></h3>
<p>The key formula is the actual re-formulation of our noising process. The authors formulate each step in the noising process with: q(x_t-1|x_t,x_0). They make sure this always adhere to the training objective, which relies on q(x_t|x_0) to be constant. This de-noising step is basically saying that we should get the next step by factoring in some of x_t and some of x_0, and then some noise (which we can scale).</p>
<p>The authors then, allow us to scale how much of q(x_t) factors into q(x_t-1), vs. how much random-noise is factored in. This basically allows us the <em>keep</em> some of the original noise; on one extreme (DDPM) we don&rsquo;t keep any of the original noise. On the other, we always keep it, and we can exactly predict x_t-1 from x_t.</p>

<img src="/images/posts/test_ipynb/images/form1.png">
<p>&gt; Intuitively, since we dont replace the &lsquo;randomness&rsquo;, we can deterministically sample from our model, and similar noises should have small variations, since we don&rsquo;t continually add-back noise!</p>
<p>The authors then go on to prove that the training objectives are the same between DDIM and DDPM.</p>
<p>Then we can take our formulation of the changed-denoising process, and begin to denoise!</p>

<img src="/images/posts/test_ipynb/images/form2.png">
<p>as we can see, we use some of the previous noise (however much we pick), and some deterministic sampling. If we&rsquo;d wanted to &lsquo;interpolate&rsquo; in the latent-space, we&rsquo;d ideally choose a low noise (œÉ) score. when œÉ is zero, we have a DDIM model.</p>
<h3 id="accelerated-generation-process">&lsquo;Accelerated&rsquo; Generation Process<a hidden class="anchor" aria-hidden="true" href="#accelerated-generation-process">#</a></h3>
<p>Since we&rsquo;ve discovered that we can formulate the inference (denosing) in any way, as long as it matches the training-objective, we can define a shorter noising-process, as long as it matches q(x_t | x_0) = same as DDPM with full T.</p>
<p>Bascially, we train with an arbitrarily large number of sampling-steps, but we actually only inference with a small subset of these steps. And, an intution for this, is that we use the same predicted x_0 to just denoise less-gradually. This way, we can train fast, while still doing most of the diffusion-work (since the model is conditioned to predict the best x_0 anyway).</p>
<h3 id="inference">Inference<a hidden class="anchor" aria-hidden="true" href="#inference">#</a></h3>
<p>Lastly, before we inference, we can just define our œÉ hyper-parameter, and then define our standard deviation as follows (for any number of steps we decide to characterize our distribution with):</p>

<img src="/images/posts/test_ipynb/images/form3.png">
<h3 id="learned-variance">Learned-Variance<a hidden class="anchor" aria-hidden="true" href="#learned-variance">#</a></h3>
<p>While our learned-variance is only for the next-step, we can actually use it to model any arbitrary jump-between steps, as what it is, is just expressing the uncertainty about the image, and the proportion of lower-to-upper bound.</p>

<img src="/images/posts/test_ipynb/images/form9.png">
<p>&gt; This intuition for this, is that when we skip an arbitary step, our next alpha/beta is just the proportion of their cumulative products, which shows &lsquo;how much&rsquo; alpha/beta we jumped. Similarly, this can be used to find the lower/upper-products.</p>
<p><em><strong>Although we may not directly use the variance in our DDIM, learning to predict the variance can force the model to be more expressive, and indirectly help the generation-process.</strong></em></p>
<h4 id="now-let"><em><strong>Now, let&rsquo;s test out this new formulation of inference on our trained DiT model!</strong></em><a hidden class="anchor" aria-hidden="true" href="#now-let">#</a></h4>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>noncom <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(start<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0001</span>,end<span style="color:#f92672">=</span><span style="color:#ae81ff">0.002</span>,steps<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>alphas <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumprod(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>noncom,dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>alphas
</span></span></code></pre></div>
</details>
</div>
<p>
 

 <style>
    .code_output img{
        text-align: center;
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
    }
    .code_output details{
        white-space: pre-wrap !important;
        word-wrap: break-word !important;
        flex-grow: 1;
        padding: 0px;
      }

      

 </style>
 <div class="code_output">
 <pre><code><details class="code_output" open><summary>Output:</summary>
tensor([0.9999, 0.9998, 0.9996, 0.9995, 0.9993, 0.9991, 0.9989, 0.9987, 0.9984,
        0.9981, 0.9978, 0.9975, 0.9972, 0.9969, 0.9965, 0.9961, 0.9957, 0.9953,
        0.9948, 0.9944, 0.9939, 0.9934, 0.9929, 0.9923, 0.9918, 0.9912, 0.9906,
        0.9900, 0.9894, 0.9887, 0.9880, 0.9874, 0.9867, 0.9859, 0.9852, 0.9844,
        0.9836, 0.9829, 0.9820, 0.9812, 0.9804, 0.9795, 0.9786, 0.9777, 0.9768,
        0.9758, 0.9749, 0.9739, 0.9729, 0.9719, 0.9709, 0.9698, 0.9687, 0.9677,
        0.9666, 0.9654, 0.9643, 0.9632, 0.9620, 0.9608, 0.9596, 0.9584, 0.9571,
        0.9559, 0.9546, 0.9533, 0.9520, 0.9507, 0.9494, 0.9480, 0.9467, 0.9453,
        0.9439, 0.9425, 0.9410, 0.9396, 0.9381, 0.9366, 0.9351, 0.9336, 0.9321,
        0.9306, 0.9290, 0.9274, 0.9258, 0.9242, 0.9226, 0.9210, 0.9193, 0.9177,
        0.9160, 0.9143, 0.9126, 0.9109, 0.9091, 0.9074, 0.9056, 0.9039, 0.9021,
        0.9003])
</details></code></pre>
</div>
<br></p>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># formulation of DDIM</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># will add to sampler class</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;Given our prediction of the noise (which we gen x_0 from), 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">and the previous sample, we get the next step, and add in some
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">random noise (if œÉ != 0 and we&#39;re not using DDIM).
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">For now, recreating alphas/betas cumprod. In the DDIM paper, alphas were treated
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">as the cumprod, so we&#39;ll just do the same.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">We can specify the total number of steps, and lower this
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">when we decide to sample with a shorter chain.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TODO: Make variance alligned with skipping-steps for fast-inference (and mean!!!) -&amp;gt; w/ pred-var!</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DDIM</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self,device,total_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,betas<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1e-4</span>,<span style="color:#ae81ff">2e-2</span>],n<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,inference_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,weight<span style="color:#f92672">=</span><span style="color:#ae81ff">1.5</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># getting alpha (which is cumprod?)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>device<span style="color:#f92672">=</span>device
</span></span><span style="display:flex;"><span>        betas <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(betas[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>weight),betas[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>weight),total_steps,device<span style="color:#f92672">=</span>device,dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)<span style="color:#f92672">**</span>(weight)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alphas_cumprod <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cumprod(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>betas,dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>inference_steps <span style="color:#f92672">=</span> inference_steps
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>total_steps <span style="color:#f92672">=</span> total_steps
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n <span style="color:#f92672">=</span> n<span style="color:#75715e"># NOT IN USE</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># model should be in eval mode,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># using it to remove noise</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># at specified steps.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># stop at last step (use)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># obtained x_0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">denoise</span>(self,model,label,image <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        step_size <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>total_steps <span style="color:#f92672">//</span> self<span style="color:#f92672">.</span>inference_steps
</span></span><span style="display:flex;"><span>        all_steps <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>,self<span style="color:#f92672">.</span>total_steps,device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        steps <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>flip(all_steps[::step_size],dims<span style="color:#f92672">=</span>[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> image <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            image <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">32</span>,<span style="color:#ae81ff">32</span>],device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> range(len(steps)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># viewing with extra b-dim</span>
</span></span><span style="display:flex;"><span>            current_t <span style="color:#f92672">=</span> steps[s]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>            eps_theta,pred_var <span style="color:#f92672">=</span> model(image,current_t,label)<span style="color:#f92672">.</span>chunk(<span style="color:#ae81ff">2</span>,dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            eps_theta,pred_var <span style="color:#f92672">=</span> eps_theta<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">1</span>),pred_var<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># &#39;prev&#39;/next step in denoise</span>
</span></span><span style="display:flex;"><span>            alpha_next_cumprod <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>alphas_cumprod[steps[s<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]]
</span></span><span style="display:flex;"><span>            alpha_current_cumprod <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>alphas_cumprod[steps[s]]
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># std-formula</span>
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># beta lower/upepr for steps</span>
</span></span><span style="display:flex;"><span>            beta_upper <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>(alpha_current_cumprod<span style="color:#f92672">/</span>alpha_next_cumprod)
</span></span><span style="display:flex;"><span>            beta_lower <span style="color:#f92672">=</span> ((<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha_next_cumprod)<span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha_current_cumprod)) <span style="color:#f92672">*</span> beta_upper
</span></span><span style="display:flex;"><span>            std <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sqrt(torch<span style="color:#f92672">.</span>exp(pred_var<span style="color:#f92672">*</span>beta_upper<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>pred_var)<span style="color:#f92672">*</span>beta_lower<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)))<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>n
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># predicting our ground-truth from noise...</span>
</span></span><span style="display:flex;"><span>            pred_x0 <span style="color:#f92672">=</span> alpha_next_cumprod<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> ((image<span style="color:#f92672">-</span>((<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha_current_cumprod<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>))<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span><span style="color:#f92672">*</span>eps_theta))<span style="color:#f92672">/</span>alpha_current_cumprod<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>            dir_xt <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>alpha_next_cumprod<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>)<span style="color:#f92672">-</span>std<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">**</span><span style="color:#ae81ff">0.5</span> <span style="color:#f92672">*</span> eps_theta
</span></span><span style="display:flex;"><span>            rand_noise <span style="color:#f92672">=</span> std<span style="color:#f92672">*</span>torch<span style="color:#f92672">.</span>randn_like(pred_x0,device<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># predicted image, doing again!</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> s<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span> <span style="color:#f92672">==</span> len(steps):
</span></span><span style="display:flex;"><span>                <span style="color:#75715e"># in case last-step</span>
</span></span><span style="display:flex;"><span>                image <span style="color:#f92672">=</span> pred_x0
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                image <span style="color:#f92672">=</span> pred_x0<span style="color:#f92672">+</span>dir_xt<span style="color:#f92672">+</span>rand_noise
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> image
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        
</span></span></code></pre></div>
</details>
</div>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> huggingface_hub
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sys
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>abspath(<span style="color:#e6db74">&#39;../data&#39;</span>))
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>abspath(<span style="color:#e6db74">&#39;../VAE&#39;</span>))
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>abspath(<span style="color:#e6db74">&#39;../DDPM&#39;</span>))
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>abspath(<span style="color:#e6db74">&#39;../DiT&#39;</span>))
</span></span><span style="display:flex;"><span>sys<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>append(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>abspath(<span style="color:#e6db74">&#39;../config&#39;</span>))
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;../config/config.yaml&#39;</span>) <span style="color:#66d9ef">as</span> file:
</span></span><span style="display:flex;"><span>    config <span style="color:#f92672">=</span> yaml<span style="color:#f92672">.</span>safe_load(file)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> dataloader
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> get_vae
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> sampler
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> transformer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>betas<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1e-4</span>,<span style="color:#ae81ff">2e-2</span>]
</span></span><span style="display:flex;"><span>steps <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> dataloader<span style="color:#f92672">.</span>ImageDataset(<span style="color:#66d9ef">None</span>,<span style="color:#e6db74">&#39;raw_images&#39;</span>,
</span></span><span style="display:flex;"><span>                                  label_folder<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;../data/label_folder&#39;</span>,
</span></span><span style="display:flex;"><span>                                  im_extension<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;jpeg&#39;</span>,
</span></span><span style="display:flex;"><span>                                  use_latents<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>                                  latent_folder<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;../data/latent_folder_sdxl&#39;</span>,
</span></span><span style="display:flex;"><span>                                  device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>)
</span></span><span style="display:flex;"><span>full_vae, encoder <span style="color:#f92672">=</span> get_vae<span style="color:#f92672">.</span>get_vae()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># diffusion-process</span>
</span></span><span style="display:flex;"><span>path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>abspath(<span style="color:#e6db74">&#39;../weights.pt&#39;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> transformer<span style="color:#f92672">.</span>DiT(config[<span style="color:#e6db74">&#39;dit_params&#39;</span>])
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>load_state_dict(torch<span style="color:#f92672">.</span>load(path))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>image_noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn([<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">4</span>,<span style="color:#ae81ff">32</span>,<span style="color:#ae81ff">32</span>],device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>gen_model <span style="color:#f92672">=</span> DDIM(device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cuda&#39;</span>,total_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,betas<span style="color:#f92672">=</span>betas,n<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>,inference_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>,weight<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>full_vae<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;done&#34;</span>)
</span></span></code></pre></div>
</details>
</div>
<p>
 

 <style>
    .code_output img{
        text-align: center;
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
    }
    .code_output details{
        white-space: pre-wrap !important;
        word-wrap: break-word !important;
        flex-grow: 1;
        padding: 0px;
      }

      

 </style>
 <div class="code_output">
 <pre><code><details class="code_output" open><summary>Output:</summary>
c:\Users\charl\OneDrive\Machine Learning\DiT Implementation\data\dataloader.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.images = torch.load(full_path,map_location=torch.device(self.device))[0]
C:\Users\charl\AppData\Local\Temp\ipykernel_26512\2425790028.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(path))

done

</details></code></pre>
</div>
<br></p>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># image of tench!</span>
</span></span><span style="display:flex;"><span>label <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">0</span>])<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    img_gen <span style="color:#f92672">=</span> gen_model<span style="color:#f92672">.</span>denoise(model<span style="color:#f92672">=</span>model,image<span style="color:#f92672">=</span>image_noise,label<span style="color:#f92672">=</span>label)
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> full_vae<span style="color:#f92672">.</span>decode(img_gen)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>cpu()
</span></span><span style="display:flex;"><span>img <span style="color:#f92672">=</span> img<span style="color:#f92672">.</span>squeeze()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>imshow(img<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Label = tench&#34;</span>)
</span></span></code></pre></div>
</details>
</div>
<p>
 

 <style>
    .code_output img{
        text-align: center;
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
    }
    .code_output details{
        white-space: pre-wrap !important;
        word-wrap: break-word !important;
        flex-grow: 1;
        padding: 0px;
      }

      

 </style>
 <div class="code_output">
 <pre><code><details class="code_output" open><summary>Output:</summary>
c:\Users\charl\miniconda3\envs\manimtest\Lib\site-packages\diffusers\models\attention_processor.py:2358: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\cb\pytorch_1000000000000\work\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:555.)
  hidden_states = F.scaled_dot_product_attention(
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9599836..2.2283664].

Text(0.5, 1.0, 'Label = tench')
&lt;Figure size 640x480 with 1 Axes&gt;


<img src="/images/posts/test_ipynb/cell_15_img.png"></details></code></pre>
</div>
<br></p>
<style>
  @keyframes textRainbow {
    0% { color: red; }
    16.7% { color: orange; }
    33.3% { color: yellow; }
    50% { color: green; }
    66.7% { color: rgb(0, 204, 255); }
    83.3% { color: rgb(0, 124, 130); }
    100% { color: violet; }
  }
  
    summary.test {
      animation: textRainbow 30s linear infinite;
      color: white;  
      padding: 10px;
      border-radius: 5px;  
      text-align: left;  
      cursor: pointer;  
      background: #131313;
      font-family: 'SF Mono', 'Cascadia Code', Menlo, monospace;
      font-size: medium;
    }

    .rain_container {
      overflow-x: scroll;
      white-space: nowrap;
      margin: 0px;
    }

    .rain_det {
      overflow-x: scroll;
      white-space: nowrap;
    
    }


  </style>


<div class="rain_container">
<details open class="rain_det">
    <summary class="test">code</summary>
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>label_map <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">0</span>:<span style="color:#e6db74">&#34;Tench&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">1</span>:<span style="color:#e6db74">&#34;English Springer&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">2</span>: <span style="color:#e6db74">&#34;Cassette Player&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">3</span>:<span style="color:#e6db74">&#34;Chain Saw&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">4</span>:<span style="color:#e6db74">&#34;Church&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">5</span>:<span style="color:#e6db74">&#34;French Horn&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">6</span>:<span style="color:#e6db74">&#34;Garabage Truck&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">7</span>:<span style="color:#e6db74">&#34;Gas Pump&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">8</span>:<span style="color:#e6db74">&#34;Golf Ball&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">9</span>:<span style="color:#e6db74">&#34;Parachute&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>fig,axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>fig<span style="color:#f92672">.</span>set_size_inches(<span style="color:#ae81ff">20</span>,<span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i,l <span style="color:#f92672">in</span> enumerate(label_map<span style="color:#f92672">.</span>keys()):
</span></span><span style="display:flex;"><span>    label <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([i])<span style="color:#f92672">.</span>to(<span style="color:#e6db74">&#39;cuda&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        img_gen <span style="color:#f92672">=</span> gen_model<span style="color:#f92672">.</span>denoise(model<span style="color:#f92672">=</span>model,image<span style="color:#f92672">=</span>image_noise,label<span style="color:#f92672">=</span>label)
</span></span><span style="display:flex;"><span>        img <span style="color:#f92672">=</span> full_vae<span style="color:#f92672">.</span>decode(img_gen)<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>        axes[i<span style="color:#f92672">//</span><span style="color:#ae81ff">5</span>][i<span style="color:#f92672">%</span><span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>imshow(img<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>))
</span></span><span style="display:flex;"><span>        axes[i<span style="color:#f92672">//</span><span style="color:#ae81ff">5</span>][i<span style="color:#f92672">%</span><span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Label: &#34;</span><span style="color:#e6db74">{</span>label_map[l]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;&#39;</span>)
</span></span><span style="display:flex;"><span>        axes[i<span style="color:#f92672">//</span><span style="color:#ae81ff">5</span>][i<span style="color:#f92672">%</span><span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
</span></span></code></pre></div>
</details>
</div>
<p>
 

 <style>
    .code_output img{
        text-align: center;
        display: block;
        margin-left: auto;
        margin-right: auto;
        max-width: 100%;
    }
    .code_output details{
        white-space: pre-wrap !important;
        word-wrap: break-word !important;
        flex-grow: 1;
        padding: 0px;
      }

      

 </style>
 <div class="code_output">
 <pre><code><details class="code_output" open><summary>Output:</summary>
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.9599836..2.2283664].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1600142..2.0967958].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3258142..2.292558].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2665067..2.3878798].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2685826..2.2057047].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2243443..2.132538].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3359764..2.325954].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3178966..2.3716161].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.2280133..1.9921035].
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.3169994..2.3565075].

&lt;Figure size 2000x1000 with 10 Axes&gt;


<img src="/images/posts/test_ipynb/cell_16_img.png"></details></code></pre>
</div>
<br></p>
<h1 id="guidance">Guidance<a hidden class="anchor" aria-hidden="true" href="#guidance">#</a></h1>
<p>We can train the model to take some text tokens (Ideally from a pre-trained model), or some other embedding (like a class embedding) for the model to learn to generate images conditionally. This is mathematically just a conditional distribution, and the model will learn it like it would learn to approximate another distribution (in the DiT we make the class embeddings play a role in AdaLN params).</p>
<h3 id="cfg">CFG<a hidden class="anchor" aria-hidden="true" href="#cfg">#</a></h3>
<p>Classifier-free guidance is a way to beef-up the capability of our model; we can, through each de-noising step during inference do the following:</p>
<ul>
<li>Apply formula:</li>
</ul>




  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer><script src="https://utteranc.es/client.js"
        repo="chuckles201/chuckles201.github.io"
        issue-term="pathname"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>

        
</script>
  
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Charlie&#39;s Blog</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
